//! Three-lane priority scheduler.
//!
//! The scheduler uses three lanes:
//! 1. Cancel lane (highest priority) - tasks with pending cancellation
//! 2. Timed lane (EDF) - tasks with deadlines
//! 3. Ready lane - all other ready tasks
//!
//! Within each lane, tasks are ordered by their priority (or deadline).
//! Uses binary heaps for O(log n) insertion instead of O(n) VecDeque insertion.

use crate::types::{TaskId, Time};
use crate::util::{ArenaIndex, DetBuildHasher, DetHashSet};
use std::cmp::Ordering;
use std::collections::hash_map::DefaultHasher;
use std::collections::BinaryHeap;
use std::hash::{Hash, Hasher};

/// A task entry in a scheduler lane ordered by priority.
///
/// Ordering: higher priority first, then earlier generation (FIFO within same priority).
#[derive(Debug, Clone, Eq, PartialEq)]
struct SchedulerEntry {
    task: TaskId,
    priority: u8,
    /// Insertion order for FIFO tie-breaking among equal priorities.
    generation: u64,
}

impl Ord for SchedulerEntry {
    fn cmp(&self, other: &Self) -> Ordering {
        // Higher priority first (BinaryHeap is max-heap)
        // For equal priorities, earlier generation (lower number) comes first
        self.priority
            .cmp(&other.priority)
            .then_with(|| other.generation.cmp(&self.generation))
    }
}

impl PartialOrd for SchedulerEntry {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

/// A task entry in a scheduler lane ordered by deadline (EDF).
///
/// Ordering: earlier deadline first, then earlier generation (FIFO within same deadline).
#[derive(Debug, Clone, Eq, PartialEq)]
struct TimedEntry {
    task: TaskId,
    deadline: Time,
    /// Insertion order for FIFO tie-breaking among equal deadlines.
    generation: u64,
}

impl Ord for TimedEntry {
    fn cmp(&self, other: &Self) -> Ordering {
        // Earlier deadline first (reverse comparison for min-heap behavior via max-heap)
        // For equal deadlines, earlier generation comes first
        other
            .deadline
            .cmp(&self.deadline)
            .then_with(|| other.generation.cmp(&self.generation))
    }
}

impl PartialOrd for TimedEntry {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

#[derive(Debug)]
struct ScheduledSet {
    // Fast path: for the common case where task IDs are dense (arena-backed),
    // store a generation tag per index to avoid hashing.
    //
    // Tag encoding:
    // - 0 => not scheduled
    // - (gen as u64) + 1 => scheduled with that generation
    // - DENSE_COLLISION => membership tracked in `overflow`
    dense: Vec<u64>,
    overflow: DetHashSet<TaskId>,
    len: usize,
}

impl ScheduledSet {
    const DENSE_COLLISION: u64 = u64::MAX;
    // Hard cap to avoid pathological allocations if someone schedules a very high-index TaskId.
    const MAX_DENSE_LEN: usize = 1 << 20; // 1,048,576 slots => 8 MiB
    const MIN_DENSE_LEN: usize = 1024;

    #[inline]
    fn with_capacity(capacity: usize) -> Self {
        let mut overflow = DetHashSet::with_hasher(DetBuildHasher);
        overflow.reserve(capacity);

        let dense_len = capacity
            .max(1)
            .next_power_of_two()
            .clamp(Self::MIN_DENSE_LEN, Self::MAX_DENSE_LEN);
        Self {
            dense: vec![0; dense_len],
            overflow,
            len: 0,
        }
    }

    #[inline]
    fn len(&self) -> usize {
        self.len
    }

    #[inline]
    fn is_empty(&self) -> bool {
        self.len == 0
    }

    #[inline]
    fn insert(&mut self, task: TaskId) -> bool {
        let idx = task.0.index() as usize;
        let tag = u64::from(task.0.generation()) + 1;

        if idx < Self::MAX_DENSE_LEN && idx >= self.dense.len() {
            self.grow_dense_to_fit(idx);
        }

        if idx >= self.dense.len() {
            // Out of dense range: fall back to deterministic hashing.
            let inserted = self.overflow.insert(task);
            if inserted {
                self.len += 1;
            }
            return inserted;
        }

        match self.dense[idx] {
            0 => {
                self.dense[idx] = tag;
                self.len += 1;
                true
            }
            existing if existing == tag => false,
            Self::DENSE_COLLISION => {
                let inserted = self.overflow.insert(task);
                if inserted {
                    self.len += 1;
                }
                inserted
            }
            existing => {
                // Collision on arena index across generations. This should be extremely rare in a
                // correct runtime (it implies re-use while still scheduled), but we preserve exact
                // set semantics by moving this index to overflow tracking.
                self.dense[idx] = Self::DENSE_COLLISION;
                let old_gen = u32::try_from(existing - 1).expect("dense tag fits u32");
                let old_task = TaskId(ArenaIndex::new(
                    u32::try_from(idx).expect("idx fits u32"),
                    old_gen,
                ));
                let was_new = self.overflow.insert(old_task);
                debug_assert!(was_new);

                let inserted = self.overflow.insert(task);
                if inserted {
                    self.len += 1;
                }
                inserted
            }
        }
    }

    #[inline]
    fn remove(&mut self, task: TaskId) -> bool {
        let idx = task.0.index() as usize;
        let tag = u64::from(task.0.generation()) + 1;

        if idx >= self.dense.len() {
            let removed = self.overflow.remove(&task);
            if removed {
                self.len -= 1;
            }
            return removed;
        }

        match self.dense[idx] {
            0 => false,
            existing if existing == tag => {
                self.dense[idx] = 0;
                self.len -= 1;
                true
            }
            Self::DENSE_COLLISION => {
                let removed = self.overflow.remove(&task);
                if removed {
                    self.len -= 1;
                }
                removed
            }
            _ => false,
        }
    }

    #[inline]
    fn clear(&mut self) {
        for slot in &mut self.dense {
            *slot = 0;
        }
        self.overflow.clear();
        self.len = 0;
    }

    #[inline]
    fn grow_dense_to_fit(&mut self, idx: usize) {
        debug_assert!(idx < Self::MAX_DENSE_LEN);
        let needed = idx + 1;
        let mut new_len = self.dense.len().max(1);
        while new_len < needed {
            new_len = new_len.saturating_mul(2);
        }
        new_len = new_len.clamp(Self::MIN_DENSE_LEN, Self::MAX_DENSE_LEN);
        if new_len > self.dense.len() {
            self.dense.resize(new_len, 0);
        }
    }
}

/// The three-lane scheduler.
///
/// Uses binary heaps for O(log n) insertion instead of O(n) VecDeque insertion.
/// Generation counters provide FIFO ordering within same priority/deadline.
#[derive(Debug)]
pub struct Scheduler {
    /// Cancel lane: tasks with pending cancellation (highest priority).
    cancel_lane: BinaryHeap<SchedulerEntry>,
    /// Timed lane: tasks with deadlines (EDF ordering).
    timed_lane: BinaryHeap<TimedEntry>,
    /// Ready lane: general ready tasks.
    ready_lane: BinaryHeap<SchedulerEntry>,
    /// Set of tasks currently in the scheduler (for dedup).
    scheduled: ScheduledSet,
    /// Next generation number for FIFO ordering.
    next_generation: u64,
    /// Scratch space for RNG tie-breaking (ready/cancel lanes).
    scratch_entries: Vec<SchedulerEntry>,
    /// Scratch space for RNG tie-breaking (timed lane).
    scratch_timed: Vec<TimedEntry>,
}

const DEFAULT_SCHEDULER_CAPACITY: usize = 256;
const DEFAULT_SCRATCH_CAPACITY: usize = 8;

impl Default for Scheduler {
    fn default() -> Self {
        Self::with_capacity(DEFAULT_SCHEDULER_CAPACITY)
    }
}

impl Scheduler {
    /// Creates a new empty scheduler.
    #[must_use]
    pub fn new() -> Self {
        Self::default()
    }

    /// Creates a scheduler with pre-allocated capacity for lanes and dedup set.
    ///
    /// The capacity is applied per lane to reduce heap growth on bursty workloads.
    #[must_use]
    pub fn with_capacity(capacity: usize) -> Self {
        let capacity = capacity.max(1);
        Self {
            cancel_lane: BinaryHeap::with_capacity(capacity),
            timed_lane: BinaryHeap::with_capacity(capacity),
            ready_lane: BinaryHeap::with_capacity(capacity),
            scheduled: ScheduledSet::with_capacity(capacity),
            next_generation: 0,
            scratch_entries: Vec::with_capacity(DEFAULT_SCRATCH_CAPACITY),
            scratch_timed: Vec::with_capacity(DEFAULT_SCRATCH_CAPACITY),
        }
    }

    /// Returns the total number of scheduled tasks.
    #[must_use]
    pub fn len(&self) -> usize {
        self.scheduled.len()
    }

    /// Returns true if no tasks are scheduled.
    #[must_use]
    pub fn is_empty(&self) -> bool {
        self.scheduled.is_empty()
    }

    /// Allocates and returns the next generation number for FIFO ordering.
    fn next_gen(&mut self) -> u64 {
        let gen = self.next_generation;
        self.next_generation += 1;
        gen
    }

    /// Schedules a task in the ready lane.
    ///
    /// Does nothing if the task is already scheduled.
    /// O(log n) insertion via binary heap.
    pub fn schedule(&mut self, task: TaskId, priority: u8) {
        if self.scheduled.insert(task) {
            let generation = self.next_gen();
            self.ready_lane.push(SchedulerEntry {
                task,
                priority,
                generation,
            });
        }
    }

    /// Schedules or promotes a task into the cancel lane.
    ///
    /// If the task is already scheduled, it is moved to the cancel lane to
    /// ensure cancellation preempts timed/ready work.
    /// O(log n) insertion for new tasks; O(n) for promotions.
    pub fn schedule_cancel(&mut self, task: TaskId, priority: u8) {
        if self.scheduled.insert(task) {
            let generation = self.next_gen();
            self.cancel_lane.push(SchedulerEntry {
                task,
                priority,
                generation,
            });
            return;
        }
        self.move_to_cancel_lane(task, priority);
    }

    /// Schedules a task in the timed lane.
    ///
    /// Does nothing if the task is already scheduled.
    /// O(log n) insertion via binary heap.
    pub fn schedule_timed(&mut self, task: TaskId, deadline: Time) {
        if self.scheduled.insert(task) {
            let generation = self.next_gen();
            self.timed_lane.push(TimedEntry {
                task,
                deadline,
                generation,
            });
        }
    }

    /// Pops the next task to run.
    ///
    /// Order: cancel lane > timed lane > ready lane.
    /// O(log n) pop via binary heap.
    pub fn pop(&mut self) -> Option<TaskId> {
        if let Some(entry) = self.cancel_lane.pop() {
            self.scheduled.remove(entry.task);
            return Some(entry.task);
        }

        if let Some(entry) = self.timed_lane.pop() {
            self.scheduled.remove(entry.task);
            return Some(entry.task);
        }

        if let Some(entry) = self.ready_lane.pop() {
            self.scheduled.remove(entry.task);
            return Some(entry.task);
        }

        None
    }

    /// Pops the next task to run, using `rng_hint` for tie-breaking among equal-priority tasks.
    ///
    /// Order: cancel lane > timed lane > ready lane.
    /// O(log n) pop via binary heap.
    pub fn pop_with_rng_hint(&mut self, rng_hint: u64) -> Option<TaskId> {
        self.pop_with_lane(rng_hint).map(|(task, _)| task)
    }

    /// Pop the highest-priority task across all three lanes, returning both
    /// the task and the lane it was dispatched from.
    ///
    /// Lane priority: Cancel > Timed > Ready (same as `pop_with_rng_hint`).
    pub fn pop_with_lane(&mut self, rng_hint: u64) -> Option<(TaskId, DispatchLane)> {
        // For lab determinism, we want tie-breaking to vary with a seed while still being fully
        // deterministic for a given `rng_hint` sequence. We do this by selecting uniformly among
        // the set of max-priority (or earliest-deadline) tasks in the chosen lane.
        if let Some(entry) =
            Self::pop_entry_with_rng(&mut self.cancel_lane, rng_hint, &mut self.scratch_entries)
        {
            self.scheduled.remove(entry.task);
            return Some((entry.task, DispatchLane::Cancel));
        }

        if let Some(entry) =
            Self::pop_timed_with_rng(&mut self.timed_lane, rng_hint, &mut self.scratch_timed)
        {
            self.scheduled.remove(entry.task);
            return Some((entry.task, DispatchLane::Timed));
        }

        if let Some(entry) =
            Self::pop_entry_with_rng(&mut self.ready_lane, rng_hint, &mut self.scratch_entries)
        {
            self.scheduled.remove(entry.task);
            return Some((entry.task, DispatchLane::Ready));
        }

        None
    }

    /// Pop a task from the cancel lane using deterministic RNG tie-breaking.
    pub fn pop_cancel_with_rng(&mut self, rng_hint: u64) -> Option<(TaskId, DispatchLane)> {
        let entry =
            Self::pop_entry_with_rng(&mut self.cancel_lane, rng_hint, &mut self.scratch_entries)?;
        self.scheduled.remove(entry.task);
        Some((entry.task, DispatchLane::Cancel))
    }

    /// Pop a task from timed or ready lanes (excluding cancel lane).
    ///
    /// Timed lane has priority over ready lane.
    pub fn pop_non_cancel_with_rng(&mut self, rng_hint: u64) -> Option<(TaskId, DispatchLane)> {
        if let Some(entry) =
            Self::pop_timed_with_rng(&mut self.timed_lane, rng_hint, &mut self.scratch_timed)
        {
            self.scheduled.remove(entry.task);
            return Some((entry.task, DispatchLane::Timed));
        }

        if let Some(entry) =
            Self::pop_entry_with_rng(&mut self.ready_lane, rng_hint, &mut self.scratch_entries)
        {
            self.scheduled.remove(entry.task);
            return Some((entry.task, DispatchLane::Ready));
        }

        None
    }

    fn pop_entry_with_rng(
        lane: &mut BinaryHeap<SchedulerEntry>,
        rng_hint: u64,
        scratch: &mut Vec<SchedulerEntry>,
    ) -> Option<SchedulerEntry> {
        let first = lane.pop()?;
        let priority = first.priority;
        scratch.clear();
        scratch.push(first);

        while let Some(peek) = lane.peek() {
            if peek.priority != priority {
                break;
            }
            // `peek` guarantees the next `pop` is `Some`.
            scratch.push(lane.pop().expect("popped after peek"));
        }

        let idx = (rng_hint as usize) % scratch.len();
        let chosen = scratch.swap_remove(idx);
        for entry in scratch.drain(..) {
            lane.push(entry);
        }
        Some(chosen)
    }

    fn pop_timed_with_rng(
        lane: &mut BinaryHeap<TimedEntry>,
        rng_hint: u64,
        scratch: &mut Vec<TimedEntry>,
    ) -> Option<TimedEntry> {
        let first = lane.pop()?;
        let deadline = first.deadline;
        scratch.clear();
        scratch.push(first);

        while let Some(peek) = lane.peek() {
            if peek.deadline != deadline {
                break;
            }
            scratch.push(lane.pop().expect("popped after peek"));
        }

        let idx = (rng_hint as usize) % scratch.len();
        let chosen = scratch.swap_remove(idx);
        for entry in scratch.drain(..) {
            lane.push(entry);
        }
        Some(chosen)
    }

    /// Removes a specific task from the scheduler.
    ///
    /// O(n) rebuild of affected lane. This is acceptable since removal is rare
    /// compared to schedule/pop operations.
    pub fn remove(&mut self, task: TaskId) {
        if self.scheduled.remove(task) {
            // Rebuild heaps without the removed task
            self.cancel_lane = self
                .cancel_lane
                .drain()
                .filter(|e| e.task != task)
                .collect();
            self.timed_lane = self.timed_lane.drain().filter(|e| e.task != task).collect();
            self.ready_lane = self.ready_lane.drain().filter(|e| e.task != task).collect();
        }
    }

    /// Moves a task to the cancel lane (highest priority).
    ///
    /// If the task is not currently scheduled, it will be added to the cancel lane.
    /// If the task is already in the cancel lane, its priority may be updated.
    ///
    /// This is the key operation for ensuring cancelled tasks get priority:
    /// the cancel lane is always drained before timed and ready lanes.
    ///
    /// O(n) for finding and removing from other lanes, O(log n) for insertion.
    pub fn move_to_cancel_lane(&mut self, task: TaskId, priority: u8) {
        let generation = self.next_gen();

        if self.scheduled.insert(task) {
            // Not scheduled, add directly to cancel lane
            self.cancel_lane.push(SchedulerEntry {
                task,
                priority,
                generation,
            });
            return;
        }

        // Task is scheduled. Check where it is.
        // Check if already in cancel lane
        let in_cancel = self.cancel_lane.iter().any(|e| e.task == task);
        if in_cancel {
            // Rebuild cancel lane, updating priority if higher
            self.cancel_lane = self
                .cancel_lane
                .drain()
                .map(|e| {
                    if e.task == task && priority > e.priority {
                        SchedulerEntry {
                            task,
                            priority,
                            generation,
                        }
                    } else {
                        e
                    }
                })
                .collect();
            return;
        }

        // Check timed lane
        let in_timed = self.timed_lane.iter().any(|e| e.task == task);
        if in_timed {
            self.timed_lane = self.timed_lane.drain().filter(|e| e.task != task).collect();
            self.cancel_lane.push(SchedulerEntry {
                task,
                priority,
                generation,
            });
            return;
        }

        // Check ready lane
        let in_ready = self.ready_lane.iter().any(|e| e.task == task);
        if in_ready {
            self.ready_lane = self.ready_lane.drain().filter(|e| e.task != task).collect();
            self.cancel_lane.push(SchedulerEntry {
                task,
                priority,
                generation,
            });
            return;
        }

        // Task is in `scheduled` set but not in any lane - add to cancel lane
        self.cancel_lane.push(SchedulerEntry {
            task,
            priority,
            generation,
        });
    }

    /// Returns true if a task is in the cancel lane.
    #[must_use]
    pub fn is_in_cancel_lane(&self, task: TaskId) -> bool {
        self.cancel_lane.iter().any(|e| e.task == task)
    }

    /// Pops only from the cancel lane.
    ///
    /// Use this for strict cancel-first processing in multi-worker scenarios.
    /// O(log n) pop via binary heap.
    #[must_use]
    pub fn pop_cancel_only(&mut self) -> Option<TaskId> {
        if let Some(entry) = self.cancel_lane.pop() {
            self.scheduled.remove(entry.task);
            return Some(entry.task);
        }
        None
    }

    /// Pops only from the cancel lane with RNG tie-breaking.
    ///
    /// Note: With heap-based implementation, FIFO ordering is used instead of RNG.
    #[must_use]
    pub fn pop_cancel_only_with_hint(&mut self, _rng_hint: u64) -> Option<TaskId> {
        self.pop_cancel_only()
    }

    /// Pops only from the timed lane if the earliest deadline is due.
    ///
    /// Returns `None` if no timed tasks exist or the earliest deadline
    /// has not yet been reached. This prevents timed tasks from firing
    /// before their deadline when in the local scheduler.
    ///
    /// O(log n) pop via binary heap.
    #[must_use]
    pub fn pop_timed_only(&mut self, now: Time) -> Option<TaskId> {
        if let Some(entry) = self.timed_lane.peek() {
            if entry.deadline <= now {
                let entry = self.timed_lane.pop().expect("peeked entry should exist");
                self.scheduled.remove(entry.task);
                return Some(entry.task);
            }
        }
        None
    }

    /// Pops only from the timed lane if the earliest deadline is due,
    /// with RNG tie-breaking.
    ///
    /// Note: With heap-based implementation, FIFO ordering is used instead of RNG.
    #[must_use]
    pub fn pop_timed_only_with_hint(&mut self, _rng_hint: u64, now: Time) -> Option<TaskId> {
        self.pop_timed_only(now)
    }

    /// Pops only from the ready lane.
    ///
    /// Use this for strict lane ordering in multi-worker scenarios.
    /// O(log n) pop via binary heap.
    #[must_use]
    pub fn pop_ready_only(&mut self) -> Option<TaskId> {
        if let Some(entry) = self.ready_lane.pop() {
            self.scheduled.remove(entry.task);
            return Some(entry.task);
        }
        None
    }

    /// Pops only from the ready lane with RNG tie-breaking.
    ///
    /// Note: With heap-based implementation, FIFO ordering is used instead of RNG.
    #[must_use]
    pub fn pop_ready_only_with_hint(&mut self, _rng_hint: u64) -> Option<TaskId> {
        self.pop_ready_only()
    }

    /// Steals a batch of ready tasks for another worker.
    ///
    /// Only steals from the ready lane to preserve cancel/timed priority semantics.
    /// Returns the stolen tasks with their priorities.
    ///
    /// O(k log n) where k is the number of tasks stolen.
    pub fn steal_ready_batch(&mut self, max_steal: usize) -> Vec<(TaskId, u8)> {
        let mut stolen = Vec::new();
        let _ = self.steal_ready_batch_into(max_steal, &mut stolen);
        stolen
    }

    /// Steals ready tasks into a caller-provided buffer.
    ///
    /// Returns the number of tasks stolen.
    pub fn steal_ready_batch_into(
        &mut self,
        max_steal: usize,
        out: &mut Vec<(TaskId, u8)>,
    ) -> usize {
        out.clear();
        if max_steal == 0 || self.ready_lane.is_empty() {
            return 0;
        }
        let steal_count = (self.ready_lane.len() / 2).min(max_steal).max(1);
        if out.capacity() < steal_count {
            out.reserve(steal_count - out.capacity());
        }

        for _ in 0..steal_count {
            if let Some(entry) = self.ready_lane.pop() {
                self.scheduled.remove(entry.task);
                out.push((entry.task, entry.priority));
            } else {
                break;
            }
        }

        out.len()
    }

    /// Returns true if the cancel lane has pending tasks.
    #[must_use]
    pub fn has_cancel_work(&self) -> bool {
        !self.cancel_lane.is_empty()
    }

    /// Returns true if the timed lane has pending tasks.
    #[must_use]
    pub fn has_timed_work(&self) -> bool {
        !self.timed_lane.is_empty()
    }

    /// Returns true if the ready lane has pending tasks.
    #[must_use]
    pub fn has_ready_work(&self) -> bool {
        !self.ready_lane.is_empty()
    }

    /// Clears all scheduled tasks.
    pub fn clear(&mut self) {
        self.cancel_lane.clear();
        self.timed_lane.clear();
        self.ready_lane.clear();
        self.scheduled.clear();
    }
}

/// Scheduler operating mode.
///
/// Controls whether the scheduler uses deterministic or throughput-optimized
/// scheduling. The deterministic mode is used by the lab runtime for
/// reproducible testing; the throughput mode is used in production.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Default)]
pub enum SchedulerMode {
    /// Deterministic mode: same seed → identical schedule.
    ///
    /// Uses RNG-seeded tie-breaking for reproducibility. Suitable for:
    /// - Lab runtime testing
    /// - DPOR exploration
    /// - Replay debugging
    /// - Proof-carrying trace generation
    #[default]
    Deterministic,

    /// Throughput mode: optimized for wall-clock performance.
    ///
    /// May use non-deterministic optimizations (e.g., batch wakeups,
    /// relaxed ordering). Not suitable for DPOR or replay.
    Throughput,
}

/// A schedule certificate: a hash of the sequence of scheduling decisions.
///
/// Two runs with the same seed should produce identical certificates if the
/// scheduler is deterministic. A divergence in certificates indicates
/// non-determinism or a bug.
///
/// # Construction
///
/// The certificate is built incrementally by hashing each scheduling decision:
/// - Task ID popped
/// - Lane from which it was popped (cancel=0, timed=1, ready=2, stolen=3)
/// - Step number
///
/// # Verification
///
/// To verify determinism, run the same test twice with the same seed and
/// compare certificates. Divergence at step N means the schedule diverged
/// at that point.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ScheduleCertificate {
    /// Running hash of all schedule decisions.
    hash: u64,
    /// Number of decisions recorded.
    decisions: u64,
    /// Step at which the first decision diverged from a reference (if any).
    divergence_step: Option<u64>,
}

/// The lane from which a task was dispatched.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum DispatchLane {
    /// Task was in cancellation state.
    Cancel,
    /// Task had a deadline.
    Timed,
    /// Task was in the general ready queue.
    Ready,
    /// Task was stolen from another worker.
    Stolen,
}

impl ScheduleCertificate {
    /// Creates a new empty certificate.
    #[must_use]
    pub fn new() -> Self {
        Self {
            hash: 0,
            decisions: 0,
            divergence_step: None,
        }
    }

    /// Record a scheduling decision: task dispatched from a lane at a step.
    pub fn record(&mut self, task: TaskId, lane: DispatchLane, step: u64) {
        let mut hasher = DefaultHasher::new();
        self.hash.hash(&mut hasher);
        // Pack the arena index for deterministic hashing.
        let idx = task.0;
        (idx.index(), idx.generation()).hash(&mut hasher);
        lane.hash(&mut hasher);
        step.hash(&mut hasher);
        self.hash = hasher.finish();
        self.decisions += 1;
    }

    /// Returns the current certificate hash.
    #[must_use]
    pub fn hash(&self) -> u64 {
        self.hash
    }

    /// Returns the number of decisions recorded.
    #[must_use]
    pub fn decisions(&self) -> u64 {
        self.decisions
    }

    /// Compare with a reference certificate and detect divergence.
    ///
    /// Returns `true` if the certificates match.
    #[must_use]
    pub fn matches(&self, other: &Self) -> bool {
        self.hash == other.hash && self.decisions == other.decisions
    }

    /// Mark a divergence at the given step.
    pub fn mark_divergence(&mut self, step: u64) {
        if self.divergence_step.is_none() {
            self.divergence_step = Some(step);
        }
    }

    /// Returns the step at which divergence was first detected.
    #[must_use]
    pub fn divergence_step(&self) -> Option<u64> {
        self.divergence_step
    }
}

impl Default for ScheduleCertificate {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::test_utils::init_test_logging;
    use crate::util::ArenaIndex;

    fn init_test(name: &str) {
        init_test_logging();
        crate::test_phase!(name);
    }

    fn task(n: u32) -> TaskId {
        TaskId::from_arena(ArenaIndex::new(n, 0))
    }

    #[test]
    fn cancel_lane_has_priority() {
        init_test("cancel_lane_has_priority");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 100);
        sched.schedule_cancel(task(2), 50);

        // Cancel lane should come first despite lower priority
        let first = sched.pop();
        let second = sched.pop();
        crate::assert_with_log!(
            first == Some(task(2)),
            "cancel lane pops first",
            Some(task(2)),
            first
        );
        crate::assert_with_log!(
            second == Some(task(1)),
            "ready lane pops second",
            Some(task(1)),
            second
        );
        crate::test_complete!("cancel_lane_has_priority");
    }

    #[test]
    fn dedup_prevents_double_schedule() {
        init_test("dedup_prevents_double_schedule");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 100);
        sched.schedule(task(1), 100);

        crate::assert_with_log!(
            sched.len() == 1,
            "duplicate schedule is deduped",
            1usize,
            sched.len()
        );
        crate::test_complete!("dedup_prevents_double_schedule");
    }

    #[test]
    fn move_to_cancel_lane_from_ready() {
        init_test("move_to_cancel_lane_from_ready");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        sched.schedule(task(2), 100);

        // Move task 2 to cancel lane
        sched.move_to_cancel_lane(task(2), 100);

        // Task 2 should come first now (cancel lane priority)
        let first = sched.pop();
        let second = sched.pop();
        crate::assert_with_log!(
            first == Some(task(2)),
            "moved task pops first",
            Some(task(2)),
            first
        );
        crate::assert_with_log!(
            second == Some(task(1)),
            "remaining ready task pops next",
            Some(task(1)),
            second
        );
        crate::test_complete!("move_to_cancel_lane_from_ready");
    }

    #[test]
    fn move_to_cancel_lane_from_timed() {
        init_test("move_to_cancel_lane_from_timed");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        sched.schedule_timed(task(2), Time::from_secs(10));

        // Move task 2 to cancel lane
        sched.move_to_cancel_lane(task(2), 100);

        // Task 2 should come first now (cancel lane priority)
        let first = sched.pop();
        let second = sched.pop();
        crate::assert_with_log!(
            first == Some(task(2)),
            "moved timed task pops first",
            Some(task(2)),
            first
        );
        crate::assert_with_log!(
            second == Some(task(1)),
            "ready task pops second",
            Some(task(1)),
            second
        );
        crate::test_complete!("move_to_cancel_lane_from_timed");
    }

    #[test]
    fn move_to_cancel_lane_unscheduled_task() {
        init_test("move_to_cancel_lane_unscheduled_task");
        let mut sched = Scheduler::new();

        // Move unscheduled task to cancel lane
        sched.move_to_cancel_lane(task(1), 100);

        crate::assert_with_log!(
            sched.len() == 1,
            "unscheduled task inserted",
            1usize,
            sched.len()
        );
        crate::assert_with_log!(
            sched.is_in_cancel_lane(task(1)),
            "task is in cancel lane",
            true,
            sched.is_in_cancel_lane(task(1))
        );
        let first = sched.pop();
        crate::assert_with_log!(
            first == Some(task(1)),
            "cancel lane pops task",
            Some(task(1)),
            first
        );
        crate::test_complete!("move_to_cancel_lane_unscheduled_task");
    }

    #[test]
    fn move_to_cancel_lane_updates_priority() {
        init_test("move_to_cancel_lane_updates_priority");
        let mut sched = Scheduler::new();
        sched.schedule_cancel(task(1), 50);
        sched.schedule_cancel(task(2), 100);

        // Move task 1 to cancel lane with higher priority
        sched.move_to_cancel_lane(task(1), 150);

        // Task 1 should now come first due to higher priority
        let first = sched.pop();
        let second = sched.pop();
        crate::assert_with_log!(
            first == Some(task(1)),
            "higher priority task pops first",
            Some(task(1)),
            first
        );
        crate::assert_with_log!(
            second == Some(task(2)),
            "lower priority task pops next",
            Some(task(2)),
            second
        );
        crate::test_complete!("move_to_cancel_lane_updates_priority");
    }

    #[test]
    fn is_in_cancel_lane() {
        init_test("is_in_cancel_lane");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        sched.schedule_cancel(task(2), 100);

        crate::assert_with_log!(
            !sched.is_in_cancel_lane(task(1)),
            "ready task not in cancel lane",
            false,
            sched.is_in_cancel_lane(task(1))
        );
        crate::assert_with_log!(
            sched.is_in_cancel_lane(task(2)),
            "cancel task is in cancel lane",
            true,
            sched.is_in_cancel_lane(task(2))
        );
        crate::test_complete!("is_in_cancel_lane");
    }

    #[test]
    fn timed_lane_edf_ordering() {
        init_test("timed_lane_edf_ordering");
        let mut sched = Scheduler::new();

        // Schedule task 1 with later deadline (T=100)
        sched.schedule_timed(task(1), Time::from_secs(100));

        // Schedule task 2 with earlier deadline (T=10)
        sched.schedule_timed(task(2), Time::from_secs(10));

        // Task 2 should come first (EDF)
        let first = sched.pop();
        let second = sched.pop();
        crate::assert_with_log!(
            first == Some(task(2)),
            "earlier deadline pops first",
            Some(task(2)),
            first
        );
        crate::assert_with_log!(
            second == Some(task(1)),
            "later deadline pops second",
            Some(task(1)),
            second
        );
        crate::test_complete!("timed_lane_edf_ordering");
    }

    #[test]
    fn timed_lane_priority_over_ready() {
        init_test("timed_lane_priority_over_ready");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 255); // Highest priority ready
        sched.schedule_timed(task(2), Time::from_secs(100)); // Timed

        // Timed lane should come before ready lane
        let first = sched.pop();
        let second = sched.pop();
        crate::assert_with_log!(
            first == Some(task(2)),
            "timed lane pops before ready",
            Some(task(2)),
            first
        );
        crate::assert_with_log!(
            second == Some(task(1)),
            "ready lane pops after timed",
            Some(task(1)),
            second
        );
        crate::test_complete!("timed_lane_priority_over_ready");
    }

    #[test]
    fn cancel_lane_priority_over_timed() {
        init_test("cancel_lane_priority_over_timed");
        let mut sched = Scheduler::new();
        sched.schedule_timed(task(1), Time::from_secs(10)); // Urgent deadline
        sched.schedule_cancel(task(2), 1); // Low priority cancel

        // Cancel lane should still come first
        let first = sched.pop();
        let second = sched.pop();
        crate::assert_with_log!(
            first == Some(task(2)),
            "cancel lane pops before timed",
            Some(task(2)),
            first
        );
        crate::assert_with_log!(
            second == Some(task(1)),
            "timed lane pops after cancel",
            Some(task(1)),
            second
        );
        crate::test_complete!("cancel_lane_priority_over_timed");
    }

    // ========== Additional Three-Lane Tests ==========

    #[test]
    fn test_three_lane_push_pop_basic() {
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        assert_eq!(sched.pop(), Some(task(1)));
        assert_eq!(sched.pop(), None);
    }

    #[test]
    fn test_three_lane_fifo_ordering() {
        let mut sched = Scheduler::new();
        // Same priority, should be FIFO
        sched.schedule(task(1), 50);
        sched.schedule(task(2), 50);
        sched.schedule(task(3), 50);

        assert_eq!(sched.pop(), Some(task(1)), "first in, first out");
        assert_eq!(sched.pop(), Some(task(2)));
        assert_eq!(sched.pop(), Some(task(3)));
    }

    #[test]
    fn test_three_lane_priority_lanes_strict() {
        let mut sched = Scheduler::new();
        // Add in reverse order
        sched.schedule(task(1), 100); // ready
        sched.schedule_timed(task(2), Time::from_secs(1)); // timed
        sched.schedule_cancel(task(3), 50); // cancel

        // Strict ordering: cancel > timed > ready
        assert_eq!(sched.pop(), Some(task(3)), "cancel first");
        assert_eq!(sched.pop(), Some(task(2)), "timed second");
        assert_eq!(sched.pop(), Some(task(1)), "ready last");
    }

    #[test]
    fn test_three_lane_empty_detection() {
        let mut sched = Scheduler::new();
        assert!(sched.is_empty());

        sched.schedule(task(1), 50);
        assert!(!sched.is_empty());

        sched.pop();
        assert!(sched.is_empty());
    }

    #[test]
    fn test_three_lane_length_tracking() {
        let mut sched = Scheduler::new();
        assert_eq!(sched.len(), 0);

        sched.schedule(task(1), 50);
        sched.schedule_cancel(task(2), 50);
        sched.schedule_timed(task(3), Time::from_secs(1));

        assert_eq!(sched.len(), 3);

        sched.pop();
        assert_eq!(sched.len(), 2);
    }

    #[test]
    fn test_cancel_lane_priority_ordering() {
        let mut sched = Scheduler::new();
        sched.schedule_cancel(task(1), 50);
        sched.schedule_cancel(task(2), 100); // higher priority
        sched.schedule_cancel(task(3), 75);

        assert_eq!(sched.pop(), Some(task(2)), "highest priority first");
        assert_eq!(sched.pop(), Some(task(3)), "middle priority second");
        assert_eq!(sched.pop(), Some(task(1)), "lowest priority last");
    }

    #[test]
    fn test_ready_lane_priority_ordering() {
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        sched.schedule(task(2), 100);
        sched.schedule(task(3), 75);

        assert_eq!(sched.pop(), Some(task(2)), "highest priority first");
        assert_eq!(sched.pop(), Some(task(3)), "middle priority second");
        assert_eq!(sched.pop(), Some(task(1)), "lowest priority last");
    }

    #[test]
    fn test_steal_ready_batch_basic() {
        let mut sched = Scheduler::new();
        for i in 0..8 {
            sched.schedule(task(i), 50);
        }

        let stolen = sched.steal_ready_batch(4);
        assert!(!stolen.is_empty());
        assert!(stolen.len() <= 4);

        // Verify stolen tasks have correct format
        for (task_id, priority) in &stolen {
            assert_eq!(*priority, 50);
            assert!(task_id.0.index() < 8);
        }
    }

    #[test]
    fn test_steal_only_from_ready() {
        let mut sched = Scheduler::new();
        sched.schedule_cancel(task(1), 100);
        sched.schedule_timed(task(2), Time::from_secs(1));
        sched.schedule(task(3), 50);

        let stolen = sched.steal_ready_batch(10);
        // Only ready task should be stolen
        assert_eq!(stolen.len(), 1);
        assert_eq!(stolen[0].0, task(3));

        // Cancel and timed should still be in scheduler
        assert!(sched.has_cancel_work());
        assert!(sched.has_timed_work());
    }

    #[test]
    fn test_pop_only_methods() {
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        sched.schedule_cancel(task(2), 100);
        sched.schedule_timed(task(3), Time::from_secs(1));

        // pop_cancel_only should only get cancel task
        assert_eq!(sched.pop_cancel_only(), Some(task(2)));
        assert_eq!(sched.pop_cancel_only(), None);

        // pop_timed_only should only get timed task (deadline is 1s, so pass now >= 1s)
        let now = Time::from_secs(1);
        assert_eq!(sched.pop_timed_only(now), Some(task(3)));
        assert_eq!(sched.pop_timed_only(now), None);

        // pop_ready_only should only get ready task
        assert_eq!(sched.pop_ready_only(), Some(task(1)));
        assert_eq!(sched.pop_ready_only(), None);
    }

    #[test]
    fn test_remove_from_scheduler() {
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        sched.schedule(task(2), 50);
        sched.schedule(task(3), 50);

        sched.remove(task(2));

        assert_eq!(sched.len(), 2);
        assert_eq!(sched.pop(), Some(task(1)));
        assert_eq!(sched.pop(), Some(task(3)));
    }

    #[test]
    fn test_clear_scheduler() {
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        sched.schedule_cancel(task(2), 100);
        sched.schedule_timed(task(3), Time::from_secs(1));

        sched.clear();

        assert!(sched.is_empty());
        assert_eq!(sched.len(), 0);
        assert!(!sched.has_cancel_work());
        assert!(!sched.has_timed_work());
        assert!(!sched.has_ready_work());
    }

    #[test]
    fn test_has_work_methods() {
        let mut sched = Scheduler::new();
        assert!(!sched.has_cancel_work());
        assert!(!sched.has_timed_work());
        assert!(!sched.has_ready_work());

        sched.schedule(task(1), 50);
        assert!(sched.has_ready_work());

        sched.schedule_cancel(task(2), 100);
        assert!(sched.has_cancel_work());

        sched.schedule_timed(task(3), Time::from_secs(1));
        assert!(sched.has_timed_work());
    }

    #[test]
    fn test_high_volume_scheduling() {
        let mut sched = Scheduler::new();
        let count = 1000;

        for i in 0..count {
            sched.schedule(task(i), (i % 256) as u8);
        }

        assert_eq!(sched.len(), count as usize);

        let mut popped = 0;
        while sched.pop().is_some() {
            popped += 1;
        }

        assert_eq!(popped, count);
        assert!(sched.is_empty());
    }

    // ── ScheduleCertificate tests ───────────────────────────────────────

    #[test]
    fn certificate_empty() {
        let cert = ScheduleCertificate::new();
        assert_eq!(cert.decisions(), 0);
        assert_eq!(cert.divergence_step(), None);
    }

    #[test]
    fn certificate_deterministic_same_sequence() {
        let mut c1 = ScheduleCertificate::new();
        let mut c2 = ScheduleCertificate::new();

        c1.record(task(1), DispatchLane::Ready, 0);
        c1.record(task(2), DispatchLane::Cancel, 1);
        c1.record(task(3), DispatchLane::Timed, 2);

        c2.record(task(1), DispatchLane::Ready, 0);
        c2.record(task(2), DispatchLane::Cancel, 1);
        c2.record(task(3), DispatchLane::Timed, 2);

        assert!(c1.matches(&c2));
        assert_eq!(c1.hash(), c2.hash());
        assert_eq!(c1.decisions(), 3);
    }

    #[test]
    fn certificate_different_sequences_diverge() {
        let mut c1 = ScheduleCertificate::new();
        let mut c2 = ScheduleCertificate::new();

        c1.record(task(1), DispatchLane::Ready, 0);
        c1.record(task(2), DispatchLane::Ready, 1);

        c2.record(task(2), DispatchLane::Ready, 0);
        c2.record(task(1), DispatchLane::Ready, 1);

        assert!(!c1.matches(&c2));
    }

    #[test]
    fn certificate_lane_matters() {
        let mut c1 = ScheduleCertificate::new();
        let mut c2 = ScheduleCertificate::new();

        c1.record(task(1), DispatchLane::Ready, 0);
        c2.record(task(1), DispatchLane::Cancel, 0);

        assert!(!c1.matches(&c2));
    }

    #[test]
    fn certificate_divergence_tracking() {
        let mut cert = ScheduleCertificate::new();
        cert.record(task(1), DispatchLane::Ready, 0);
        assert_eq!(cert.divergence_step(), None);

        cert.mark_divergence(5);
        assert_eq!(cert.divergence_step(), Some(5));

        // First divergence is sticky.
        cert.mark_divergence(10);
        assert_eq!(cert.divergence_step(), Some(5));
    }

    #[test]
    fn scheduler_mode_default_is_deterministic() {
        assert_eq!(SchedulerMode::default(), SchedulerMode::Deterministic);
    }

    // ── pop_with_lane tests ───────────────────────────────────────────────

    #[test]
    fn pop_with_lane_returns_cancel_lane() {
        init_test("pop_with_lane_returns_cancel_lane");
        let mut sched = Scheduler::new();
        sched.schedule_cancel(task(1), 100);

        let result = sched.pop_with_lane(0);
        crate::assert_with_log!(
            result == Some((task(1), DispatchLane::Cancel)),
            "cancel task dispatches from Cancel lane",
            Some((task(1), DispatchLane::Cancel)),
            result
        );
        crate::test_complete!("pop_with_lane_returns_cancel_lane");
    }

    #[test]
    fn pop_with_lane_returns_timed_lane() {
        init_test("pop_with_lane_returns_timed_lane");
        let mut sched = Scheduler::new();
        sched.schedule_timed(task(1), Time::from_secs(10));

        let result = sched.pop_with_lane(0);
        crate::assert_with_log!(
            result == Some((task(1), DispatchLane::Timed)),
            "timed task dispatches from Timed lane",
            Some((task(1), DispatchLane::Timed)),
            result
        );
        crate::test_complete!("pop_with_lane_returns_timed_lane");
    }

    #[test]
    fn pop_with_lane_returns_ready_lane() {
        init_test("pop_with_lane_returns_ready_lane");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);

        let result = sched.pop_with_lane(0);
        crate::assert_with_log!(
            result == Some((task(1), DispatchLane::Ready)),
            "ready task dispatches from Ready lane",
            Some((task(1), DispatchLane::Ready)),
            result
        );
        crate::test_complete!("pop_with_lane_returns_ready_lane");
    }

    #[test]
    fn pop_with_lane_respects_lane_ordering() {
        init_test("pop_with_lane_respects_lane_ordering");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);
        sched.schedule_timed(task(2), Time::from_secs(10));
        sched.schedule_cancel(task(3), 10);

        let first = sched.pop_with_lane(0);
        let second = sched.pop_with_lane(0);
        let third = sched.pop_with_lane(0);
        let fourth = sched.pop_with_lane(0);

        crate::assert_with_log!(
            first.map(|(_, l)| l) == Some(DispatchLane::Cancel),
            "cancel dispatches first",
            Some(DispatchLane::Cancel),
            first.map(|(_, l)| l)
        );
        crate::assert_with_log!(
            second.map(|(_, l)| l) == Some(DispatchLane::Timed),
            "timed dispatches second",
            Some(DispatchLane::Timed),
            second.map(|(_, l)| l)
        );
        crate::assert_with_log!(
            third.map(|(_, l)| l) == Some(DispatchLane::Ready),
            "ready dispatches third",
            Some(DispatchLane::Ready),
            third.map(|(_, l)| l)
        );
        crate::assert_with_log!(
            fourth.is_none(),
            "empty scheduler returns None",
            Option::<(TaskId, DispatchLane)>::None,
            fourth
        );
        crate::test_complete!("pop_with_lane_respects_lane_ordering");
    }

    #[test]
    fn pop_with_lane_rng_tiebreak_among_equal_priority() {
        init_test("pop_with_lane_rng_tiebreak_among_equal_priority");
        let mut sched = Scheduler::new();

        // Schedule 4 tasks all at priority 50
        for i in 0..4 {
            sched.schedule(task(i), 50);
        }

        // With different rng_hints, we should get different ordering
        // (since all have equal priority, rng selects among them)
        let mut results_hint_0 = Vec::new();
        let mut sched_copy = Scheduler::new();
        for i in 0..4 {
            sched_copy.schedule(task(i), 50);
        }

        for step in 0..4 {
            if let Some((t, _)) = sched.pop_with_lane(step) {
                results_hint_0.push(t);
            }
        }

        for step in 0..4 {
            if let Some((t, _)) = sched_copy.pop_with_lane(step + 42) {
                results_hint_0.push(t);
            }
        }

        // All 8 pops should succeed (4 from each scheduler)
        crate::assert_with_log!(
            results_hint_0.len() == 8,
            "all tasks dispatched from both schedulers",
            8usize,
            results_hint_0.len()
        );
        crate::test_complete!("pop_with_lane_rng_tiebreak_among_equal_priority");
    }

    // ── steal_ready_batch_into tests ──────────────────────────────────────

    #[test]
    fn steal_ready_batch_into_fills_buffer() {
        init_test("steal_ready_batch_into_fills_buffer");
        let mut sched = Scheduler::new();
        for i in 0..10 {
            sched.schedule(task(i), 50);
        }

        let mut buf = Vec::new();
        let count = sched.steal_ready_batch_into(5, &mut buf);

        crate::assert_with_log!(
            count == buf.len(),
            "returned count matches buffer length",
            count,
            buf.len()
        );
        crate::assert_with_log!(count <= 5, "does not exceed max_steal", true, count <= 5);
        crate::assert_with_log!(count > 0, "steals at least one task", true, count > 0);
        crate::test_complete!("steal_ready_batch_into_fills_buffer");
    }

    #[test]
    fn steal_ready_batch_into_does_not_steal_cancel_or_timed() {
        init_test("steal_ready_batch_into_does_not_steal_cancel_or_timed");
        let mut sched = Scheduler::new();
        sched.schedule_cancel(task(1), 100);
        sched.schedule_timed(task(2), Time::from_secs(10));

        let mut buf = Vec::new();
        let count = sched.steal_ready_batch_into(10, &mut buf);

        crate::assert_with_log!(
            count == 0,
            "nothing stolen when ready lane is empty",
            0usize,
            count
        );
        // Cancel and timed tasks should still be present
        crate::assert_with_log!(
            sched.has_cancel_work(),
            "cancel task preserved",
            true,
            sched.has_cancel_work()
        );
        crate::assert_with_log!(
            sched.has_timed_work(),
            "timed task preserved",
            true,
            sched.has_timed_work()
        );
        crate::test_complete!("steal_ready_batch_into_does_not_steal_cancel_or_timed");
    }

    #[test]
    fn steal_ready_batch_into_respects_zero_max() {
        init_test("steal_ready_batch_into_respects_zero_max");
        let mut sched = Scheduler::new();
        for i in 0..4 {
            sched.schedule(task(i), 50);
        }

        let mut buf = Vec::new();
        let count = sched.steal_ready_batch_into(0, &mut buf);

        crate::assert_with_log!(count == 0, "zero max_steal returns zero", 0usize, count);
        crate::assert_with_log!(
            buf.is_empty(),
            "buffer cleared when max_steal is zero",
            true,
            buf.is_empty()
        );
        crate::assert_with_log!(sched.len() == 4, "no tasks removed", 4usize, sched.len());
        crate::test_complete!("steal_ready_batch_into_respects_zero_max");
    }

    #[test]
    fn steal_ready_batch_into_clears_buffer() {
        init_test("steal_ready_batch_into_clears_buffer");
        let mut sched = Scheduler::new();
        sched.schedule(task(1), 50);

        let mut buf = vec![(task(99), 255)]; // Pre-existing junk
        let count = sched.steal_ready_batch_into(10, &mut buf);

        crate::assert_with_log!(count == 1, "stole exactly one task", 1usize, count);
        crate::assert_with_log!(
            buf.len() == 1,
            "buffer cleared before filling",
            1usize,
            buf.len()
        );
        crate::assert_with_log!(
            buf[0].0 == task(1),
            "correct task in buffer",
            task(1),
            buf[0].0
        );
        crate::test_complete!("steal_ready_batch_into_clears_buffer");
    }

    // ── pop_timed_only edge cases ─────────────────────────────────────────

    #[test]
    fn pop_timed_only_respects_deadline_boundary() {
        init_test("pop_timed_only_respects_deadline_boundary");
        let mut sched = Scheduler::new();
        sched.schedule_timed(task(1), Time::from_secs(100));

        // Before deadline: should not dispatch
        let before = sched.pop_timed_only(Time::from_secs(99));
        crate::assert_with_log!(
            before.is_none(),
            "timed task not due before deadline",
            Option::<TaskId>::None,
            before
        );

        // Exactly at deadline: should dispatch
        let at = sched.pop_timed_only(Time::from_secs(100));
        crate::assert_with_log!(
            at == Some(task(1)),
            "timed task dispatches at deadline",
            Some(task(1)),
            at
        );
        crate::test_complete!("pop_timed_only_respects_deadline_boundary");
    }

    #[test]
    fn pop_timed_only_edf_with_mixed_due_status() {
        init_test("pop_timed_only_edf_with_mixed_due_status");
        let mut sched = Scheduler::new();
        sched.schedule_timed(task(1), Time::from_secs(50)); // due
        sched.schedule_timed(task(2), Time::from_secs(200)); // not due
        sched.schedule_timed(task(3), Time::from_secs(75)); // due

        let now = Time::from_secs(100);

        // Should return earliest deadline first
        let first = sched.pop_timed_only(now);
        crate::assert_with_log!(
            first == Some(task(1)),
            "earliest deadline dispatches first",
            Some(task(1)),
            first
        );

        let second = sched.pop_timed_only(now);
        crate::assert_with_log!(
            second == Some(task(3)),
            "second earliest deadline dispatches next",
            Some(task(3)),
            second
        );

        // Task 2 is not due (deadline 200 > now 100)
        let third = sched.pop_timed_only(now);
        crate::assert_with_log!(
            third.is_none(),
            "not-due task is not dispatched",
            Option::<TaskId>::None,
            third
        );
        crate::test_complete!("pop_timed_only_edf_with_mixed_due_status");
    }

    // ---- Cancel preemption: cancel drains before any timed/ready --------

    #[test]
    fn cancel_drains_completely_before_timed_and_ready() {
        init_test("cancel_drains_completely_before_timed_and_ready");
        let mut sched = Scheduler::new();

        // Schedule ready, timed, and cancel tasks in mixed order.
        sched.schedule(task(1), 100);
        sched.schedule_timed(task(2), Time::from_secs(1));
        sched.schedule_cancel(task(3), 50);
        sched.schedule(task(4), 200);
        sched.schedule_cancel(task(5), 100);
        sched.schedule_timed(task(6), Time::from_secs(2));

        // First two pops must be from cancel lane.
        let (_first, lane1) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(
            matches!(lane1, DispatchLane::Cancel),
            "first from cancel",
            true,
            true
        );

        let (_second, lane2) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(
            matches!(lane2, DispatchLane::Cancel),
            "second from cancel",
            true,
            true
        );

        // Now timed lane should drain (EDF order).
        let (_third, lane3) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(
            matches!(lane3, DispatchLane::Timed),
            "third from timed",
            true,
            true
        );

        let (_fourth, lane4) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(
            matches!(lane4, DispatchLane::Timed),
            "fourth from timed",
            true,
            true
        );

        // Finally ready lane.
        let (_fifth, lane5) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(
            matches!(lane5, DispatchLane::Ready),
            "fifth from ready",
            true,
            true
        );

        let (_sixth, lane6) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(
            matches!(lane6, DispatchLane::Ready),
            "sixth from ready",
            true,
            true
        );

        // Scheduler should now be empty.
        crate::assert_with_log!(
            sched.is_empty(),
            "empty after drain",
            true,
            sched.is_empty()
        );
        crate::test_complete!("cancel_drains_completely_before_timed_and_ready");
    }

    // ---- Move to cancel preserves other ready work ----------------------

    #[test]
    fn move_to_cancel_preserves_ready_work() {
        init_test("move_to_cancel_preserves_ready_work");
        let mut sched = Scheduler::new();

        // Schedule three ready tasks.
        sched.schedule(task(1), 100);
        sched.schedule(task(2), 100);
        sched.schedule(task(3), 100);
        let len_before = sched.len();
        crate::assert_with_log!(len_before == 3, "before move", 3, len_before);

        // Move task(2) to cancel lane.
        sched.move_to_cancel_lane(task(2), 200);

        // Total count should remain 3.
        let len_after = sched.len();
        crate::assert_with_log!(len_after == 3, "after move", 3, len_after);

        // First pop should be task(2) from cancel lane.
        let (first, lane) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(first == task(2), "cancel first", task(2), first);
        crate::assert_with_log!(
            matches!(lane, DispatchLane::Cancel),
            "from cancel lane",
            true,
            true
        );

        // Remaining two should be from ready lane.
        let (_, lane2) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(
            matches!(lane2, DispatchLane::Ready),
            "second from ready",
            true,
            true
        );

        let (_, lane3) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(
            matches!(lane3, DispatchLane::Ready),
            "third from ready",
            true,
            true
        );

        crate::assert_with_log!(sched.is_empty(), "empty", true, sched.is_empty());
        crate::test_complete!("move_to_cancel_preserves_ready_work");
    }

    // ---- Interleaved schedule/pop maintains invariants -------------------

    #[test]
    fn interleaved_schedule_pop_correct() {
        init_test("interleaved_schedule_pop_correct");
        let mut sched = Scheduler::new();

        // Schedule and pop interleaved — scheduler should always return
        // highest priority lane first.
        sched.schedule(task(1), 50);
        let first = sched.pop();
        crate::assert_with_log!(first == Some(task(1)), "pop ready", Some(task(1)), first);

        sched.schedule_cancel(task(2), 100);
        sched.schedule(task(3), 200); // higher ready priority but cancel wins

        let second = sched.pop();
        crate::assert_with_log!(
            second == Some(task(2)),
            "cancel preempts",
            Some(task(2)),
            second
        );

        let third = sched.pop();
        crate::assert_with_log!(
            third == Some(task(3)),
            "ready dispatches after cancel drain",
            Some(task(3)),
            third
        );

        crate::assert_with_log!(sched.is_empty(), "empty", true, sched.is_empty());
        crate::test_complete!("interleaved_schedule_pop_correct");
    }

    // ---- EDF with many same-deadline tasks is stable --------------------

    #[test]
    fn edf_same_deadline_fifo_stable() {
        init_test("edf_same_deadline_fifo_stable");
        let mut sched = Scheduler::new();
        let deadline = Time::from_secs(100);

        // Schedule 10 tasks with the same deadline — should dispatch in FIFO order
        // (by generation) when using basic pop.
        for i in 0..10 {
            sched.schedule_timed(task(i), deadline);
        }

        let mut order = Vec::new();
        while let Some(t) = sched.pop() {
            order.push(t);
        }

        crate::assert_with_log!(order.len() == 10, "all dispatched", 10, order.len());

        // Verify FIFO ordering (earlier index = lower task number).
        for window in order.windows(2) {
            let a_idx = window[0].arena_index().index();
            let b_idx = window[1].arena_index().index();
            crate::assert_with_log!(a_idx < b_idx, "FIFO order", true, true);
        }
        crate::test_complete!("edf_same_deadline_fifo_stable");
    }

    // ---- Remove from specific lane doesn't corrupt other lanes ----------

    #[test]
    fn remove_does_not_corrupt_other_lanes() {
        init_test("remove_does_not_corrupt_other_lanes");
        let mut sched = Scheduler::new();

        sched.schedule(task(1), 100);
        sched.schedule_timed(task(2), Time::from_secs(10));
        sched.schedule_cancel(task(3), 200);

        // Remove timed task.
        sched.remove(task(2));
        let len = sched.len();
        crate::assert_with_log!(len == 2, "after remove", 2, len);

        // Cancel and ready should still work.
        let (first, lane1) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(first == task(3), "cancel intact", task(3), first);
        crate::assert_with_log!(
            matches!(lane1, DispatchLane::Cancel),
            "cancel lane",
            true,
            true
        );

        let (second, lane2) = sched.pop_with_lane(0).unwrap();
        crate::assert_with_log!(second == task(1), "ready intact", task(1), second);
        crate::assert_with_log!(
            matches!(lane2, DispatchLane::Ready),
            "ready lane",
            true,
            true
        );

        crate::assert_with_log!(sched.is_empty(), "empty", true, sched.is_empty());
        crate::test_complete!("remove_does_not_corrupt_other_lanes");
    }

    // ---- High-volume cancel/ready interleaving --------------------------

    #[test]
    fn high_volume_cancel_ready_interleaving() {
        init_test("high_volume_cancel_ready_interleaving");
        let mut sched = Scheduler::new();

        // Schedule 50 cancel + 50 ready tasks.
        for i in 0..50 {
            sched.schedule_cancel(task(i), 100);
        }
        for i in 50..100 {
            sched.schedule(task(i), 100);
        }

        let total = sched.len();
        crate::assert_with_log!(total == 100, "total", 100, total);

        // All cancel tasks must dispatch before any ready task.
        let mut cancel_count = 0;
        let mut ready_seen = false;
        while let Some((_, lane)) = sched.pop_with_lane(0) {
            match lane {
                DispatchLane::Cancel => {
                    crate::assert_with_log!(
                        !ready_seen,
                        "no ready before cancel drains",
                        true,
                        true
                    );
                    cancel_count += 1;
                }
                DispatchLane::Ready => {
                    ready_seen = true;
                }
                DispatchLane::Timed | DispatchLane::Stolen => {}
            }
        }

        crate::assert_with_log!(cancel_count == 50, "cancel count", 50, cancel_count);
        crate::assert_with_log!(ready_seen, "ready seen", true, ready_seen);
        crate::assert_with_log!(sched.is_empty(), "empty", true, sched.is_empty());
        crate::test_complete!("high_volume_cancel_ready_interleaving");
    }

    // ── Cancel promotion parity regression tests (bd-1zaql) ─────────────

    #[test]
    fn move_to_cancel_promotes_from_ready() {
        init_test("move_to_cancel_promotes_from_ready");
        let mut sched = Scheduler::new();
        let task = TaskId::new_for_test(1, 0);

        // Schedule in ready lane
        sched.schedule(task, 50);
        crate::assert_with_log!(
            !sched.is_in_cancel_lane(task),
            "not in cancel before promotion",
            true,
            true
        );

        // Promote to cancel lane
        sched.move_to_cancel_lane(task, 100);
        crate::assert_with_log!(
            sched.is_in_cancel_lane(task),
            "in cancel after promotion",
            true,
            true
        );

        // Pop should come from cancel lane
        let (popped, lane) = sched.pop_with_lane(0).expect("should have task");
        crate::assert_with_log!(popped == task, "correct task", task, popped);
        crate::assert_with_log!(
            matches!(lane, DispatchLane::Cancel),
            "dispatched from cancel lane",
            true,
            true
        );

        // Ready lane should be empty (task was removed during promotion)
        crate::assert_with_log!(
            sched.pop_ready_only().is_none(),
            "ready lane empty after promotion",
            true,
            true
        );
        crate::test_complete!("move_to_cancel_promotes_from_ready");
    }

    #[test]
    fn move_to_cancel_promotes_from_timed() {
        init_test("move_to_cancel_promotes_from_timed");
        let mut sched = Scheduler::new();
        let task = TaskId::new_for_test(2, 0);

        // Schedule in timed lane
        sched.schedule_timed(task, Time::from_nanos(5000));
        crate::assert_with_log!(
            !sched.is_in_cancel_lane(task),
            "not in cancel before promotion",
            true,
            true
        );

        // Promote to cancel lane
        sched.move_to_cancel_lane(task, 80);
        crate::assert_with_log!(
            sched.is_in_cancel_lane(task),
            "in cancel after promotion",
            true,
            true
        );

        // Pop should come from cancel lane
        let (popped, lane) = sched.pop_with_lane(0).expect("should have task");
        crate::assert_with_log!(popped == task, "correct task", task, popped);
        crate::assert_with_log!(
            matches!(lane, DispatchLane::Cancel),
            "dispatched from cancel lane",
            true,
            true
        );
        crate::test_complete!("move_to_cancel_promotes_from_timed");
    }

    #[test]
    fn move_to_cancel_idempotent_updates_priority() {
        init_test("move_to_cancel_idempotent_updates_priority");
        let mut sched = Scheduler::new();
        let task = TaskId::new_for_test(3, 0);

        // Schedule in cancel lane at low priority
        sched.schedule_cancel(task, 10);
        crate::assert_with_log!(sched.is_in_cancel_lane(task), "in cancel lane", true, true);

        // Promote again with higher priority (idempotent, updates priority)
        sched.move_to_cancel_lane(task, 200);
        crate::assert_with_log!(
            sched.is_in_cancel_lane(task),
            "still in cancel lane",
            true,
            true
        );

        // Only one task should be in scheduler
        crate::assert_with_log!(sched.len() == 1, "exactly one task", 1usize, sched.len());
        crate::test_complete!("move_to_cancel_idempotent_updates_priority");
    }

    #[test]
    fn schedule_cancel_promotes_from_ready() {
        init_test("schedule_cancel_promotes_from_ready");
        let mut sched = Scheduler::new();
        let task = TaskId::new_for_test(4, 0);

        // Schedule in ready lane
        sched.schedule(task, 50);

        // schedule_cancel should promote to cancel lane when already scheduled
        sched.schedule_cancel(task, 100);
        crate::assert_with_log!(
            sched.is_in_cancel_lane(task),
            "schedule_cancel promotes from ready",
            true,
            true
        );
        crate::test_complete!("schedule_cancel_promotes_from_ready");
    }

    #[test]
    fn repeated_cancel_requests_are_idempotent() {
        init_test("repeated_cancel_requests_are_idempotent");
        let mut sched = Scheduler::new();
        let task = TaskId::new_for_test(5, 0);

        // First cancel
        sched.move_to_cancel_lane(task, 50);
        crate::assert_with_log!(
            sched.len() == 1,
            "one task after first cancel",
            1usize,
            sched.len()
        );

        // Repeated cancel (same priority)
        sched.move_to_cancel_lane(task, 50);
        crate::assert_with_log!(
            sched.len() == 1,
            "still one task after repeat",
            1usize,
            sched.len()
        );

        // Repeated cancel (higher priority)
        sched.move_to_cancel_lane(task, 200);
        crate::assert_with_log!(
            sched.len() == 1,
            "still one task after priority bump",
            1usize,
            sched.len()
        );
        crate::test_complete!("repeated_cancel_requests_are_idempotent");
    }
}
